{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2358595c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload  \n",
    "%autoreload 2 \n",
    "\n",
    "from subset_active_learning.active_learning.subset_classifier import get_df_from_db\n",
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import transformers\n",
    "import wandb\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "765d3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_from_db(\"./sst_results.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bdc01e8",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "annealing_runs = 5000\n",
    "total_sample_size = 1000\n",
    "n_search = 100\n",
    "db_name = 'sst_results'\n",
    "anneal_factor = 0.1\n",
    "\n",
    "model_card = \"google/electra-small-discriminator\"\n",
    "pretraining = True\n",
    "max_steps = 6000\n",
    "eval_steps = 300\n",
    "learning_rate = 1e-5\n",
    "batch_size = 8\n",
    "# adam should default to correct_bias = True\n",
    "adam_epsilon = 1e-6\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "max_grad_norm = 1.0\n",
    "warmup_ratio = 0.1\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0342ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e305ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2 = datasets.load_dataset('sst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b807fe67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sst2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/garylai/Dev/subset-active-learning/scripts/subset_selection/simulated_annealing_sst.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/garylai/Dev/subset-active-learning/scripts/subset_selection/simulated_annealing_sst.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sst2\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sst2' is not defined"
     ]
    }
   ],
   "source": [
    "sst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6611ecce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def tokenize_function(examples, field='sentence'):\n",
    "    return tokenizer(examples[field], padding=False, truncation=True)\n",
    "tokenized_sst2 = sst2.map(tokenize_function, batched=True)\n",
    "lengths = [len(i) for i in tokenized_sst2['train']['input_ids']]\n",
    "max_length = np.quantile(lengths, 0.9)\n",
    "max_length = np.max(lengths)\n",
    "'''\n",
    "max_length = 66\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138b7140",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sst2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/garylai/Dev/subset-active-learning/scripts/subset_selection/simulated_annealing_sst.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/garylai/Dev/subset-active-learning/scripts/subset_selection/simulated_annealing_sst.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_function\u001b[39m(examples, field\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/garylai/Dev/subset-active-learning/scripts/subset_selection/simulated_annealing_sst.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[field], padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, max_length\u001b[39m=\u001b[39mmax_length, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/garylai/Dev/subset-active-learning/scripts/subset_selection/simulated_annealing_sst.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenized_sst2 \u001b[39m=\u001b[39m sst2\u001b[39m.\u001b[39mmap(tokenize_function, batched\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sst2' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples, field='sentence'):\n",
    "    return tokenizer(examples[field], padding='max_length', max_length=max_length, truncation=True)\n",
    "\n",
    "tokenized_sst2 = sst2.map(tokenize_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize label\n",
    "tokenized_sst2 = tokenized_sst2.rename_column('label', 'scalar_label')\n",
    "tokenized_sst2 = tokenized_sst2.map(lambda x: {'labels' : 0 if x['scalar_label'] < 0.5 else 1})\n",
    "\n",
    "print(tokenized_sst2['test'][-10:]['sentence'])\n",
    "print(tokenized_sst2['test'][-10:]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d37986",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sst2.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df946ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_downsample = tokenized_sst2['train'].shuffle(seed=seed).select(range(0, total_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cfa624",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(tokenized_sst2['validation'], shuffle=False, batch_size=batch_size, pin_memory=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(tokenized_sst2['test'], shuffle=False, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbe365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf(model_card, num_labels=2):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_card, num_labels=num_labels)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc49dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = datasets.load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aaa51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"./sst_results.db\") as conn:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM states ORDER BY objective DESC LIMIT 1 OFFSET %d\" % 2)\n",
    "    r = cursor.fetchone()\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7993fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nth_best = np.array(json.loads(r[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071303f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nth_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24e7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_idx = np.arange(total_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a67d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nth_best = np.array(\n",
    "    [\n",
    "        517,\n",
    "        882,\n",
    "        82,\n",
    "        341,\n",
    "        477,\n",
    "        381,\n",
    "        611,\n",
    "        377,\n",
    "        837,\n",
    "        217,\n",
    "        712,\n",
    "        216,\n",
    "        514,\n",
    "        728,\n",
    "        523,\n",
    "        842,\n",
    "        614,\n",
    "        316,\n",
    "        7,\n",
    "        536,\n",
    "        540,\n",
    "        636,\n",
    "        766,\n",
    "        479,\n",
    "        461,\n",
    "        85,\n",
    "        852,\n",
    "        270,\n",
    "        460,\n",
    "        3,\n",
    "        664,\n",
    "        56,\n",
    "        605,\n",
    "        315,\n",
    "        741,\n",
    "        352,\n",
    "        175,\n",
    "        112,\n",
    "        176,\n",
    "        120,\n",
    "        349,\n",
    "        485,\n",
    "        8,\n",
    "        459,\n",
    "        146,\n",
    "        627,\n",
    "        159,\n",
    "        325,\n",
    "        796,\n",
    "        888,\n",
    "        318,\n",
    "        430,\n",
    "        364,\n",
    "        467,\n",
    "        510,\n",
    "        704,\n",
    "        895,\n",
    "        496,\n",
    "        812,\n",
    "        727,\n",
    "        797,\n",
    "        184,\n",
    "        744,\n",
    "        322,\n",
    "        22,\n",
    "        669,\n",
    "        917,\n",
    "        103,\n",
    "        548,\n",
    "        631,\n",
    "        577,\n",
    "        137,\n",
    "        194,\n",
    "        111,\n",
    "        81,\n",
    "        321,\n",
    "        66,\n",
    "        228,\n",
    "        145,\n",
    "        63,\n",
    "        602,\n",
    "        24,\n",
    "        233,\n",
    "        511,\n",
    "        752,\n",
    "        681,\n",
    "        589,\n",
    "        33,\n",
    "        791,\n",
    "        6,\n",
    "        731,\n",
    "        231,\n",
    "        114,\n",
    "        101,\n",
    "        422,\n",
    "        735,\n",
    "        382,\n",
    "        23,\n",
    "        88,\n",
    "        901,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# np.random.seed(42)\n",
    "# random.randint(0, len(test_nth_best) - 1)\n",
    "\n",
    "# create_new_subset_in_place(base_subset=test_nth_best)\n",
    "# len(set(test_nth_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b341a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_nth_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7f43f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swapping out: 123 at index 51 | swapping in: 124\n"
     ]
    }
   ],
   "source": [
    "create_new_subset_in_place(base_subset=test_nth_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b0342673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "print(np.random.randint(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "90c50caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(test_nth_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "315aad7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(np.random.choice(total_sample_size, size=n_search, replace=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a3fd7278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.4 µs ± 3.89 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit create_new_subset_in_place(test_nth_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "4bf44173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def transition(idx):\n",
    "    total_idx = np.arange(total_sample_size)\n",
    "    not_in = np.setdiff1d(total_idx, idx)\n",
    "    a = np.random.choice(not_in)\n",
    "    b = random.randint(0, len(idx) - 1)\n",
    "    new_idx = idx.copy()\n",
    "    new_idx[b] = a\n",
    "    return new_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "5cb9cb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.7 µs ± 11.3 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit transition(test_nth_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d842a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.setdiff1d(total_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aeccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.choice(total_sample_size, size=n_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd49475",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca1a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    con = sqlite3.connect('%s.db' % db_name)\n",
    "    print(f\"con: {con}\")\n",
    "    cur = con.cursor()\n",
    "    cur.execute('''CREATE TABLE states\n",
    "                   (indexes text, objective real)''')\n",
    "    cur.execute('''CREATE INDEX idx_objective \n",
    "                    ON states (objective);''')\n",
    "    \n",
    "    # start from a random sample\n",
    "    random_idx = np.random.choice(total_sample_size, size=n_search, replace=False)\n",
    "    if (num_unique_samples := len(set(random_idx))) != total_sample_size:\n",
    "        raise ValueError(f\"Unexpected number of indices are selected. Expected {total_sample_size}, got {num_unique_samples}\")\n",
    "    print(\"random_idx: \", random_idx)\n",
    "    cur.execute(\"INSERT INTO states VALUES ('%s', 0)\" % json.dumps(random_idx.tolist()))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "finally:\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c25fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_best_subset(n):\n",
    "    \"\"\" Select a single example -- the nth best by test accuracy to swap out\n",
    "    \"\"\"\n",
    "    try:\n",
    "        con = sqlite3.connect('%s.db' % db_name)\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"SELECT * FROM states ORDER BY objective DESC LIMIT 1 OFFSET %d\" % n)\n",
    "        r = cur.fetchone()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return (np.array(json.loads(r[0])), r[1])\n",
    "\n",
    "def get_num_runs():\n",
    "    try:\n",
    "        con = sqlite3.connect('%s.db' % db_name)\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"SELECT COUNT(objective) FROM states\")\n",
    "        r = cur.fetchone()[0]\n",
    "        con.commit()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return r\n",
    "\n",
    "def insert_run(idx, obj):\n",
    "    try:\n",
    "        con = sqlite3.connect('%s.db' % db_name)\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"INSERT INTO states VALUES ('%s', %.8f)\" % (json.dumps(idx.tolist()), obj))\n",
    "        con.commit()\n",
    "    finally:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c90e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garylai/Dev/subset-active-learning/subset_active_learning/subset_selection/select.py:10: RuntimeWarning: fields may not start with an underscore, ignoring \"_n_gpu\"\n",
      "  class SubsetSelector(BaseModel, extra=Extra.allow):\n"
     ]
    }
   ],
   "source": [
    "from subset_active_learning.subset_selection import select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00627b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garylai/Dev/subset-active-learning/subset_active_learning/subset_selection/select.py:10: RuntimeWarning: fields may not start with an underscore, ignoring \"_n_gpu\"\n",
      "  class SubsetSelector(BaseModel, extra=Extra.allow):\n",
      "[autoreload of subset_active_learning.subset_selection.select failed: Traceback (most recent call last):\n",
      "  File \"/Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 480, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 377, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 345, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 303, in update_instances\n",
      "    ref.__class__ = new\n",
      "  File \"pydantic/main.py\", line 401, in pydantic.main.BaseModel.__setattr__\n",
      "AttributeError: __fields_set__\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "selector = select.SubsetSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc91f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sst_results'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd14a8c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'select' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/garylai/Dev/subset-active-learning/scripts/subset_selection/simulated_annealing_sst.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/garylai/Dev/subset-active-learning/scripts/subset_selection/simulated_annealing_sst.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m selector \u001b[39m=\u001b[39m select\u001b[39m.\u001b[39mSubsetSelector()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'select' is not defined"
     ]
    }
   ],
   "source": [
    "selector = select.SubsetSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a5a0dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sst_results'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d0dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666 µs ± 80.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit get_num_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "62a1fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_subset_in_place(base_subset: np.ndarray) -> None:\n",
    "    \"\"\"Create a new subset by swapping out one sample from the base_subset\n",
    "    and swapping in a new sample. This is done inplace for efficiency\n",
    "    \"\"\"\n",
    "    all_indices = np.arange(total_sample_size)\n",
    "    available_examples = np.setdiff1d(all_indices, base_subset)\n",
    "    # np.random.seed(42)\n",
    "    in_sample = np.random.choice(available_examples)\n",
    "    out_sample_idx = np.random.randint(0, len(base_subset) - 1)\n",
    "    # print(f\"swapping out: {base_subset[out_sample_idx]} at index {out_sample_idx} | swapping in: {in_sample}\")\n",
    "    base_subset[out_sample_idx] = in_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transition():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2808f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    val_pbar = tqdm(total=len(val_dataloader))\n",
    "    for batch in val_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        val_pbar.update(1)\n",
    "\n",
    "    eval_dict = metric.compute()\n",
    "    val_pbar.set_description('Acc: %.2f' % eval_dict['accuracy'])\n",
    "    \n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005081a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, tolerance=2):\n",
    "    steps = 0\n",
    "    epochs = 0\n",
    "    best_acc = None\n",
    "    patience = 0\n",
    "    pbar = tqdm(total=max_steps)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size, pin_memory=True)\n",
    "    it = iter(train_dataloader)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate, betas=(adam_beta1, adam_beta2), eps=adam_epsilon, weight_decay=weight_decay)\n",
    "    lr_scheduler = transformers.get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=warmup_ratio*max_steps, num_training_steps=max_steps)\n",
    "\n",
    "    while steps < max_steps:\n",
    "        # training\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "\n",
    "        try:\n",
    "            batch = next(it)\n",
    "        except:\n",
    "            epochs += 1\n",
    "            it = iter(train_dataloader)\n",
    "            batch = next(it)\n",
    "\n",
    "        steps += 1\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        total_loss += loss.cpu()\n",
    "\n",
    "        wandb.log({'loss' : loss})\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_description('Epoch: %d, Avg batch loss: %.2f' % (epochs, total_loss / steps))\n",
    "        pbar.update(1)\n",
    "\n",
    "        if steps % eval_steps == 0:\n",
    "            model.eval()\n",
    "            eval_dict = evaluate(model, val_dataloader)\n",
    "            wandb.log({'sst:val_acc' : eval_dict['accuracy']})\n",
    "            \n",
    "            # early stopping\n",
    "            if not best_acc or eval_dict['accuracy'] > best_acc:\n",
    "                best_acc = eval_dict['accuracy']\n",
    "            else:\n",
    "                patience += 1\n",
    "            \n",
    "            if patience >= tolerance:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7607e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def one_run():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # controls transition sampling\n",
    "    # random.seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "\n",
    "    i = get_num_runs()\n",
    "    ratio = (annealing_runs - i) / annealing_runs if i < annealing_runs else 0\n",
    "    idx = np.random.randint(0, int(ratio * i)) # 0 < idx < ratio * current_num_run; the bigger the n, the more exploration. The smaller the n, the greedier. \n",
    "    nth_best_index, quality = get_nth_best_subset(idx) # get the nth best subset (size 100)\n",
    "    \n",
    "    new_idx = create_new_subset_in_place(nth_best_index)\n",
    "    train_dataset = sst2_downsample.select(new_idx) # you're always selecting `n_search` 100 datapoints\n",
    "    assert(len(train_dataset) == n_search)\n",
    "    \n",
    "    model, tokenizer = create_hf(model_card, num_labels=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    wandb.init(project=\"simulated_annealing-sst\", entity=\"johntzwei\", tags=[model_card])\n",
    "    wandb.log({'n_downsample' : total_sample_size})\n",
    "    wandb.log({'n_search' : n_search})\n",
    "    wandb.log({'model_card' : model_card})\n",
    "    wandb.log({'indexes' : json.dumps(idx.tolist())})\n",
    "\n",
    "    train(model, train_dataset)\n",
    "    eval_dict = evaluate(model, test_dataloader)\n",
    "    eval_dict = {'sst2_test:%s' % k : v for k, v in eval_dict.items()}\n",
    "    new_quality = eval_dict['sst2_test:accuracy']\n",
    "    wandb.log(eval_dict)\n",
    "    \n",
    "    insert_run(new_idx, new_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    one_run()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
