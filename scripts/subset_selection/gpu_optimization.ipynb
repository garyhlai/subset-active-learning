{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload  \n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffcv\n",
    "from ffcv.writer import DatasetWriter\n",
    "from ffcv.fields import IntField, NDArrayField, FloatField\n",
    "import datasets\n",
    "from subset_active_learning.subset_selection import select, preprocess\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Found cached dataset sst (/home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 760.39it/s]\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-f8fd6701869c12c8.arrow\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-92e4d8b08735d65d.arrow\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-5fc3a07bc35230ee.arrow\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-631cfbbaad84f8a4.arrow\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-a55da0d6356956ba.arrow\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-9fe317e190ba186c.arrow\n"
     ]
    }
   ],
   "source": [
    "processed_ds = preprocess.preprocess_sst2(\"google/electra-small-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_PATH = \"/home/glai/dev/subset-active-learning/local_bucket\"\n",
    "write_path = f'{LOCAL_PATH}/ds.beton'\n",
    "\n",
    "# Pass a type for each data field\n",
    "writer = DatasetWriter(write_path, {\n",
    "    # Tune options to optimize dataset size, throughput at train-time\n",
    "    \"scalar_label\": FloatField(), \n",
    "    'input_ids': NDArrayField(np.dtype(np.int32), shape=(8544, 66)), \n",
    "    \"token_type_ids\": NDArrayField(np.dtype(np.int8), shape=(8544, 66)),\n",
    "    \"attention_mask\": NDArrayField(np.dtype(np.int8), shape=(8544, 66)),\n",
    "    'label': IntField()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert HF dataset into Torch dataset for ffcv support\n",
    "class SST2(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_ds):\n",
    "        self.ds = hf_ds\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.ds[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SST2(hf_ds=processed_ds[\"train\"])\n",
    "# Write dataset\n",
    "writer.from_indexed_dataset(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class BatchSizeComparisonRun: \n",
    "    def __init__(self, train_ds: datasets.Dataset, valid_ds: datasets.Dataset, test_ds: datasets.Dataset):\n",
    "        self.train_ds,self.valid_ds,self.test_ds = train_ds,valid_ds,test_ds\n",
    "\n",
    "    def _one_run(self, wandb_tag: str, config: select.SubsetTrainingArguments):\n",
    "        wandb_run = wandb.init(project=\"subset-search-gpu-opt\", entity=\"johnny-gary\", tags=[wandb_tag])\n",
    "        wandb.log({\"batch_size\": config.batch_size})\n",
    "        subset_trainer = select.SubsetTrainer(\n",
    "            params=config, valid_ds=self.valid_ds, test_ds=self.test_ds\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        subset_trainer.train_one_step(subset=self.train_ds, calculate_test_accuracy=True)\n",
    "        wandb.log({\"run_time\": round(time.time() - start_time, 2)})\n",
    "        wandb_run.finish()\n",
    "\n",
    "    def run_comparison(self, small_batch_config: select.SubsetTrainingArguments, large_batch_config: select.SubsetTrainingArguments): \n",
    "        \"\"\"\n",
    "        - train small batch size until early stopping\n",
    "        - train large batch size until early stopping\n",
    "        \"\"\"\n",
    "        self._one_run(wandb_tag=f\"small_batch_{small_batch_config.batch_size}\", config=small_batch_config)\n",
    "        self._one_run(wandb_tag=f\"large_batch_{large_batch_config.batch_size}\", config=large_batch_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCREASE_FACTOR = 4\n",
    "small_batch_config = select.SubsetTrainingArguments(batch_size=8, learning_rate=1e-5)\n",
    "large_batch_config = select.SubsetTrainingArguments(batch_size=small_batch_config.batch_size*INCREASE_FACTOR, learning_rate=small_batch_config.learning_rate*(3/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-a7e7c77d48022288.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_210947-2329bev1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/2329bev1\" target=\"_blank\">snowy-resonance-11</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.53: 100%|██████████| 138/138 [00:01<00:00, 72.41it/s]00:23<07:33, 12.57it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 69.95it/s]00:48<07:03, 12.76it/s]\n",
      "Acc: 0.70: 100%|██████████| 138/138 [00:02<00:00, 67.97it/s]01:14<06:55, 12.27it/s]\n",
      "Acc: 0.71: 100%|██████████| 138/138 [00:02<00:00, 68.87it/s][01:40<06:11, 12.92it/s]\n",
      "Acc: 0.71: 100%|██████████| 138/138 [00:01<00:00, 69.78it/s] [02:06<05:41, 13.19it/s]\n",
      "Acc: 0.71: 100%|██████████| 138/138 [00:01<00:00, 69.45it/s] [02:32<05:26, 12.87it/s]\n",
      "Acc: 0.71: 100%|██████████| 138/138 [00:02<00:00, 68.74it/s] [02:58<05:00, 12.96it/s]\n",
      "Acc: 0.72: 100%|██████████| 138/138 [00:01<00:00, 71.69it/s] [03:24<05:17, 11.36it/s]\n",
      "Acc: 0.71: 100%|██████████| 138/138 [00:02<00:00, 66.10it/s] [03:49<04:11, 13.10it/s]\n",
      "Epoch: 207, Avg batch loss: 0.00:  45%|████▌     | 2700/6000 [03:52<04:43, 11.63it/s]\n",
      "Acc: 0.72: 100%|██████████| 277/277 [00:03<00:00, 78.83it/s]\n",
      "Acc: 0.71: 100%|██████████| 138/138 [00:01<00:00, 81.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>██████▇▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>run_time</td><td>▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▆███████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>loss</td><td>0.0014</td></tr><tr><td>run_time</td><td>237.8</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.71765</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.71117</td></tr><tr><td>sst:val_acc</td><td>0.71117</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">snowy-resonance-11</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/2329bev1\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/2329bev1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_210947-2329bev1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_211352-vtl2izmf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/vtl2izmf\" target=\"_blank\">twilight-yogurt-12</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:00<00:00, 35.43it/s] [00:26<08:14, 11.52it/s]\n",
      "Acc: 0.71: 100%|██████████| 35/35 [00:00<00:00, 35.47it/s]0 [00:54<08:07, 11.08it/s]\n",
      "Acc: 0.72: 100%|██████████| 35/35 [00:00<00:00, 35.93it/s]0 [01:22<07:42, 11.03it/s]\n",
      "Acc: 0.72: 100%|██████████| 35/35 [00:00<00:00, 35.09it/s]00 [01:50<07:22, 10.86it/s]\n",
      "Acc: 0.71: 100%|██████████| 35/35 [00:01<00:00, 33.21it/s]00 [02:18<07:06, 10.56it/s]\n",
      "Epoch: 374, Avg batch loss: 0.00:  25%|██▌       | 1500/6000 [02:20<07:00, 10.71it/s]\n",
      "Acc: 0.70: 100%|██████████| 70/70 [00:01<00:00, 35.71it/s]\n",
      "Acc: 0.71: 100%|██████████| 35/35 [00:00<00:00, 36.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>██████▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>run_time</td><td>▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▇██▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>32</td></tr><tr><td>loss</td><td>0.00065</td></tr><tr><td>run_time</td><td>143.43</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.70226</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.71299</td></tr><tr><td>sst:val_acc</td><td>0.71299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">twilight-yogurt-12</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/vtl2izmf\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/vtl2izmf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_211352-vtl2izmf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-57bf3cafa2eaea20.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_211621-3qrxp8s8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3qrxp8s8\" target=\"_blank\">youthful-thunder-13</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.56: 100%|██████████| 138/138 [00:02<00:00, 67.38it/s]00:24<07:27, 12.75it/s]\n",
      "Acc: 0.65: 100%|██████████| 138/138 [00:02<00:00, 67.25it/s]00:50<07:31, 11.96it/s]\n",
      "Acc: 0.65: 100%|██████████| 138/138 [00:02<00:00, 66.15it/s]01:16<06:43, 12.64it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 69.32it/s][01:42<06:20, 12.61it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 69.83it/s] [02:07<05:57, 12.59it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:02<00:00, 62.80it/s] [02:33<05:34, 12.56it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 70.34it/s] [02:59<04:59, 13.03it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 71.16it/s] [03:25<04:46, 12.56it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 69.72it/s] [03:51<04:17, 12.84it/s]\n",
      "Epoch: 207, Avg batch loss: 0.00:  45%|████▌     | 2700/6000 [03:53<04:45, 11.56it/s]\n",
      "Acc: 0.66: 100%|██████████| 277/277 [00:03<00:00, 79.61it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 81.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>█████▇▇▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>run_time</td><td>▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▆▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>loss</td><td>0.00144</td></tr><tr><td>run_time</td><td>239.3</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.66154</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.66757</td></tr><tr><td>sst:val_acc</td><td>0.66757</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">youthful-thunder-13</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3qrxp8s8\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3qrxp8s8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_211621-3qrxp8s8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_212027-1cj9c8kr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/1cj9c8kr\" target=\"_blank\">helpful-frost-14</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.66: 100%|██████████| 35/35 [00:01<00:00, 34.30it/s] [00:26<08:32, 11.12it/s]\n",
      "Acc: 0.68: 100%|██████████| 35/35 [00:01<00:00, 34.39it/s]0 [00:54<08:19, 10.82it/s]\n",
      "Acc: 0.68: 100%|██████████| 35/35 [00:01<00:00, 34.42it/s]0 [01:23<07:34, 11.21it/s]\n",
      "Acc: 0.68: 100%|██████████| 35/35 [00:01<00:00, 34.21it/s]00 [01:51<07:21, 10.87it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:01<00:00, 33.26it/s]00 [02:19<07:16, 10.32it/s]\n",
      "Epoch: 374, Avg batch loss: 0.00:  25%|██▌       | 1500/6000 [02:20<07:02, 10.66it/s]\n",
      "Acc: 0.68: 100%|██████████| 70/70 [00:01<00:00, 37.82it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:00<00:00, 36.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>█████▆▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>run_time</td><td>▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▇█▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>32</td></tr><tr><td>loss</td><td>0.00041</td></tr><tr><td>run_time</td><td>144.08</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.67557</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.67484</td></tr><tr><td>sst:val_acc</td><td>0.67484</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">helpful-frost-14</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/1cj9c8kr\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/1cj9c8kr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_212027-1cj9c8kr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-a77dd7fd6d9be44e.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_212257-1bgt0qhu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/1bgt0qhu\" target=\"_blank\">bright-galaxy-15</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.51: 100%|██████████| 138/138 [00:01<00:00, 71.42it/s]00:23<06:56, 13.68it/s]\n",
      "Acc: 0.64: 100%|██████████| 138/138 [00:01<00:00, 71.33it/s]00:49<07:01, 12.80it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:02<00:00, 68.99it/s]01:15<06:28, 13.14it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 76.16it/s][01:41<06:07, 13.05it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 69.55it/s] [02:07<05:57, 12.60it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 69.61it/s] [02:33<05:37, 12.44it/s]\n",
      "Epoch: 138, Avg batch loss: 0.00:  30%|███       | 1800/6000 [02:35<06:02, 11.60it/s]\n",
      "Acc: 0.68: 100%|██████████| 277/277 [00:03<00:00, 79.92it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 81.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>███████▇█▇▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>run_time</td><td>▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>loss</td><td>0.00441</td></tr><tr><td>run_time</td><td>160.88</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.67602</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.66848</td></tr><tr><td>sst:val_acc</td><td>0.66848</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">bright-galaxy-15</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/1bgt0qhu\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/1bgt0qhu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_212257-1bgt0qhu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_212544-3fpsqs4o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3fpsqs4o\" target=\"_blank\">misunderstood-lion-16</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.69: 100%|██████████| 35/35 [00:00<00:00, 35.39it/s] [00:26<08:32, 11.13it/s]\n",
      "Acc: 0.69: 100%|██████████| 35/35 [00:01<00:00, 33.76it/s]0 [00:54<08:19, 10.80it/s]\n",
      "Acc: 0.69: 100%|██████████| 35/35 [00:01<00:00, 34.34it/s]0 [01:23<08:07, 10.46it/s]\n",
      "Acc: 0.72: 100%|██████████| 35/35 [00:00<00:00, 35.59it/s]00 [01:51<07:28, 10.71it/s]\n",
      "Acc: 0.71: 100%|██████████| 35/35 [00:01<00:00, 34.07it/s]00 [02:19<06:38, 11.30it/s]\n",
      "Epoch: 374, Avg batch loss: 0.00:  25%|██▌       | 1500/6000 [02:20<07:02, 10.65it/s]\n",
      "Acc: 0.73: 100%|██████████| 70/70 [00:01<00:00, 38.65it/s]\n",
      "Acc: 0.71: 100%|██████████| 35/35 [00:00<00:00, 39.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>█▇█▇▆▆▇▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>run_time</td><td>▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▃▂█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>32</td></tr><tr><td>loss</td><td>0.00049</td></tr><tr><td>run_time</td><td>144.11</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.72534</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.71208</td></tr><tr><td>sst:val_acc</td><td>0.71208</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">misunderstood-lion-16</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3fpsqs4o\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3fpsqs4o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_212544-3fpsqs4o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-ea745ee92ccd2f7a.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_212814-1eoiltld</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/1eoiltld\" target=\"_blank\">elated-oath-17</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.53: 100%|██████████| 138/138 [00:02<00:00, 67.23it/s]00:23<07:46, 12.22it/s]\n",
      "Acc: 0.63: 100%|██████████| 138/138 [00:02<00:00, 68.37it/s]00:49<06:49, 13.18it/s]\n",
      "Acc: 0.68: 100%|██████████| 138/138 [00:01<00:00, 70.13it/s]01:15<06:33, 12.95it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:02<00:00, 68.85it/s][01:41<06:23, 12.52it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 69.78it/s] [02:07<06:06, 12.29it/s]\n",
      "Epoch: 115, Avg batch loss: 0.00:  25%|██▌       | 1500/6000 [02:09<06:27, 11.62it/s]\n",
      "Acc: 0.67: 100%|██████████| 277/277 [00:03<00:00, 79.54it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 79.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>█████▇██▇▆▇▇▆▅▆▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>run_time</td><td>▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▆██▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>loss</td><td>0.00592</td></tr><tr><td>run_time</td><td>134.81</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.67376</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.6594</td></tr><tr><td>sst:val_acc</td><td>0.6594</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">elated-oath-17</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/1eoiltld\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/1eoiltld</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_212814-1eoiltld/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_213035-3l5njt0r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3l5njt0r\" target=\"_blank\">wise-fog-18</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.62: 100%|██████████| 35/35 [00:00<00:00, 36.53it/s] [00:26<08:12, 11.58it/s]\n",
      "Acc: 0.63: 100%|██████████| 35/35 [00:01<00:00, 34.94it/s]0 [00:55<08:06, 11.10it/s]\n",
      "Acc: 0.62: 100%|██████████| 35/35 [00:01<00:00, 34.29it/s]0 [01:23<07:33, 11.25it/s]\n",
      "Acc: 0.61: 100%|██████████| 35/35 [00:00<00:00, 36.50it/s]00 [01:51<07:10, 11.14it/s]\n",
      "Epoch: 299, Avg batch loss: 0.00:  20%|██        | 1200/6000 [01:52<07:29, 10.67it/s]\n",
      "Acc: 0.63: 100%|██████████| 70/70 [00:01<00:00, 40.27it/s]\n",
      "Acc: 0.61: 100%|██████████| 35/35 [00:00<00:00, 38.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>█████▇▆▅▄▃▃▂█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>run_time</td><td>▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▂█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>32</td></tr><tr><td>loss</td><td>0.00091</td></tr><tr><td>run_time</td><td>115.56</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.63439</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.6149</td></tr><tr><td>sst:val_acc</td><td>0.6149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">wise-fog-18</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3l5njt0r\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3l5njt0r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_213035-3l5njt0r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-6b25b3b1c1522c32.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_213236-jaiy0jwm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/jaiy0jwm\" target=\"_blank\">decent-violet-19</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.60: 100%|██████████| 138/138 [00:01<00:00, 75.66it/s]00:23<07:08, 13.32it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 70.06it/s]00:49<07:01, 12.81it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:02<00:00, 68.91it/s]01:14<06:41, 12.70it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:02<00:00, 67.34it/s][01:40<06:44, 11.88it/s]\n",
      "Acc: 0.68: 100%|██████████| 138/138 [00:01<00:00, 69.70it/s] [02:06<06:10, 12.16it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:02<00:00, 68.53it/s] [02:32<05:26, 12.86it/s]\n",
      "Epoch: 138, Avg batch loss: 0.00:  30%|███       | 1800/6000 [02:34<06:00, 11.64it/s]\n",
      "Acc: 0.66: 100%|██████████| 277/277 [00:03<00:00, 80.67it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 82.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>████████▇▇▆▅▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>run_time</td><td>▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▆▇▆█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>loss</td><td>0.00419</td></tr><tr><td>run_time</td><td>160.23</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.66018</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.65668</td></tr><tr><td>sst:val_acc</td><td>0.65668</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">decent-violet-19</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/jaiy0jwm\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/jaiy0jwm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_213236-jaiy0jwm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_213522-nwbn8u7l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/nwbn8u7l\" target=\"_blank\">fresh-eon-20</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.66: 100%|██████████| 35/35 [00:01<00:00, 33.42it/s] [00:26<08:22, 11.36it/s]\n",
      "Acc: 0.66: 100%|██████████| 35/35 [00:01<00:00, 33.79it/s]0 [00:54<08:06, 11.11it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:01<00:00, 33.57it/s]0 [01:22<07:49, 10.85it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:00<00:00, 35.03it/s]00 [01:50<07:23, 10.84it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:00<00:00, 35.16it/s]00 [02:19<06:25, 11.66it/s]\n",
      "Epoch: 374, Avg batch loss: 0.00:  25%|██▌       | 1500/6000 [02:20<07:00, 10.71it/s]\n",
      "Acc: 0.66: 100%|██████████| 70/70 [00:01<00:00, 38.41it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:00<00:00, 39.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>█████▇▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>run_time</td><td>▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▃█▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>32</td></tr><tr><td>loss</td><td>0.00063</td></tr><tr><td>run_time</td><td>143.23</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.65611</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.66848</td></tr><tr><td>sst:val_acc</td><td>0.66848</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fresh-eon-20</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/nwbn8u7l\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/nwbn8u7l</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_213522-nwbn8u7l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(42, 47):\n",
    "    train_ds = processed_ds[\"train\"].shuffle(seed=seed).select(range(100))\n",
    "    batch_size_comparison = BatchSizeComparisonRun(train_ds=train_ds, valid_ds=processed_ds[\"validation\"], test_ds=processed_ds[\"test\"])\n",
    "    batch_size_comparison.run_comparison(small_batch_config=small_batch_config, large_batch_config=large_batch_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ffcv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5a369980de5afa77dbc9af21c188db1feaa0541138845d75ffa63a1e0dd1c07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
