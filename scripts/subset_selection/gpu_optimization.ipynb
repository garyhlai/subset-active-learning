{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glai/anaconda3/envs/subset/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from subset_active_learning.subset_selection import select, preprocess\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Reusing dataset sst (/home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 809.55it/s]\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-7b23d5d1250b3cef.arrow\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-6a94e7b0eb5aeeeb.arrow\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-f1806eefe95f0de4.arrow\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-e283858a38c31b48.arrow\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-39bfe2cb0f239a41.arrow\n",
      "Loading cached processed dataset at /home/glai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-a9c205acb85f354d.arrow\n"
     ]
    }
   ],
   "source": [
    "processed_ds = preprocess.preprocess_sst2(\"google/electra-small-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSizeComparisonRun: \n",
    "    def __init__(self, train_ds: datasets.Dataset, valid_ds: datasets.Dataset, test_ds: datasets.Dataset):\n",
    "        self.train_ds,self.valid_ds,self.test_ds = train_ds,valid_ds,test_ds\n",
    "\n",
    "    def _one_run(self, wandb_tag: str, config: select.SubsetTrainingArguments):\n",
    "        wandb_run = wandb.init(project=\"subset-search-gpu-opt\", entity=\"johnny-gary\", tags=[wandb_tag])\n",
    "        wandb.log({\"batch_size\": config.batch_size})\n",
    "        subset_trainer = select.SubsetTrainer(\n",
    "            params=config, valid_ds=self.valid_ds, test_ds=self.test_ds\n",
    "        )\n",
    "        subset_trainer.train_one_step(subset=self.train_ds, calculate_test_accuracy=True)\n",
    "        wandb_run.finish()\n",
    "\n",
    "    def run_comparison(self, small_batch_config: select.SubsetTrainingArguments, large_batch_config: select.SubsetTrainingArguments): \n",
    "        \"\"\"\n",
    "        - train small batch size until early stopping\n",
    "        - train large batch size until early stopping\n",
    "        \"\"\"\n",
    "        self._one_run(wandb_tag=f\"small_batch_{small_batch_config.batch_size}\", config=small_batch_config)\n",
    "        self._one_run(wandb_tag=f\"large_batch_{large_batch_config.batch_size}\", config=large_batch_config)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCREASE_FACTOR = 4\n",
    "small_batch_config = select.SubsetTrainingArguments(batch_size=8, learning_rate=1e-5)\n",
    "large_batch_config = select.SubsetTrainingArguments(batch_size=small_batch_config.batch_size*INCREASE_FACTOR, learning_rate=small_batch_config.learning_rate*INCREASE_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_203817-32kumlgj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/32kumlgj\" target=\"_blank\">smart-plasma-3</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.55: 100%|██████████| 138/138 [00:01<00:00, 71.38it/s]00:24<07:44, 12.28it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:02<00:00, 67.41it/s]00:50<06:57, 12.92it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 69.10it/s]01:16<06:48, 12.49it/s]\n",
      "Acc: 0.65: 100%|██████████| 138/138 [00:02<00:00, 68.92it/s][01:42<06:21, 12.57it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 69.65it/s] [02:07<06:05, 12.31it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 70.04it/s] [02:33<05:57, 11.75it/s]\n",
      "Epoch: 138, Avg batch loss: 0.00:  30%|███       | 1800/6000 [02:35<06:03, 11.57it/s]\n",
      "Acc: 0.67: 100%|██████████| 277/277 [00:03<00:00, 79.76it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 80.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>███████▇▇█▇▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>loss</td><td>0.0034</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.66878</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.6594</td></tr><tr><td>sst:val_acc</td><td>0.6594</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">smart-plasma-3</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/32kumlgj\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/32kumlgj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_203817-32kumlgj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_204104-cig9vyna</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/cig9vyna\" target=\"_blank\">sunny-surf-4</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.65: 100%|██████████| 35/35 [00:00<00:00, 36.46it/s] [00:26<09:12, 10.32it/s]\n",
      "Acc: 0.68: 100%|██████████| 35/35 [00:00<00:00, 35.02it/s]0 [00:54<08:15, 10.90it/s]\n",
      "Acc: 0.69: 100%|██████████| 35/35 [00:00<00:00, 36.96it/s]0 [01:22<07:54, 10.75it/s]\n",
      "Acc: 0.66: 100%|██████████| 35/35 [00:01<00:00, 34.29it/s]00 [01:50<07:02, 11.35it/s]\n",
      "Acc: 0.68: 100%|██████████| 35/35 [00:01<00:00, 33.75it/s]00 [02:18<06:41, 11.20it/s]\n",
      "Epoch: 374, Avg batch loss: 0.00:  25%|██▌       | 1500/6000 [02:19<06:59, 10.72it/s]\n",
      "Acc: 0.66: 100%|██████████| 70/70 [00:01<00:00, 38.04it/s]\n",
      "Acc: 0.68: 100%|██████████| 35/35 [00:00<00:00, 39.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>███▇▇▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▇█▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>32</td></tr><tr><td>loss</td><td>0.00045</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.65656</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.67575</td></tr><tr><td>sst:val_acc</td><td>0.67575</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sunny-surf-4</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/cig9vyna\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/cig9vyna</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_204104-cig9vyna/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_204333-alyc70ef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/alyc70ef\" target=\"_blank\">usual-bird-5</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.51: 100%|██████████| 138/138 [00:01<00:00, 71.26it/s]00:24<07:27, 12.73it/s]\n",
      "Acc: 0.68: 100%|██████████| 138/138 [00:02<00:00, 67.44it/s]00:50<07:16, 12.36it/s]\n",
      "Acc: 0.68: 100%|██████████| 138/138 [00:02<00:00, 67.57it/s]01:16<06:43, 12.64it/s]\n",
      "Acc: 0.68: 100%|██████████| 138/138 [00:01<00:00, 70.97it/s][01:41<06:15, 12.77it/s]\n",
      "Epoch: 92, Avg batch loss: 0.00:  20%|██        | 1200/6000 [01:43<06:54, 11.59it/s]\n",
      "Acc: 0.70: 100%|██████████| 277/277 [00:03<00:00, 79.83it/s]\n",
      "Acc: 0.68: 100%|██████████| 138/138 [00:01<00:00, 80.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>████████▇▇▆▇█▇▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>loss</td><td>0.01054</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.7</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.6812</td></tr><tr><td>sst:val_acc</td><td>0.6812</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">usual-bird-5</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/alyc70ef\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/alyc70ef</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_204333-alyc70ef/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_204529-3ljadtj6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3ljadtj6\" target=\"_blank\">earthy-planet-6</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.64: 100%|██████████| 35/35 [00:00<00:00, 36.75it/s] [00:26<08:28, 11.21it/s]\n",
      "Acc: 0.66: 100%|██████████| 35/35 [00:00<00:00, 35.92it/s]0 [00:54<07:51, 11.45it/s]\n",
      "Acc: 0.66: 100%|██████████| 35/35 [00:01<00:00, 33.12it/s]0 [01:22<07:38, 11.13it/s]\n",
      "Acc: 0.66: 100%|██████████| 35/35 [00:00<00:00, 36.96it/s]00 [01:51<07:13, 11.07it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:00<00:00, 36.72it/s]00 [02:19<06:39, 11.27it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:01<00:00, 34.02it/s]00 [02:47<06:23, 10.95it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:01<00:00, 34.14it/s]00 [03:15<05:47, 11.22it/s]\n",
      "Acc: 0.68: 100%|██████████| 35/35 [00:01<00:00, 34.38it/s]00 [03:44<05:40, 10.58it/s]\n",
      "Acc: 0.68: 100%|██████████| 35/35 [00:01<00:00, 34.59it/s]00 [04:12<05:07, 10.75it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:01<00:00, 33.76it/s]00 [04:40<04:30, 11.07it/s]\n",
      "Epoch: 749, Avg batch loss: 0.00:  50%|█████     | 3000/6000 [04:41<04:41, 10.64it/s]\n",
      "Acc: 0.69: 100%|██████████| 70/70 [00:01<00:00, 36.15it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:00<00:00, 39.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>██▇▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▄▅▅▆▆▇██▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>32</td></tr><tr><td>loss</td><td>0.00012</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.68597</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.67393</td></tr><tr><td>sst:val_acc</td><td>0.67393</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">earthy-planet-6</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3ljadtj6\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3ljadtj6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_204529-3ljadtj6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_205020-9ym243g5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/9ym243g5\" target=\"_blank\">fearless-cherry-7</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.53: 100%|██████████| 138/138 [00:02<00:00, 66.39it/s]00:23<07:22, 12.88it/s]\n",
      "Acc: 0.64: 100%|██████████| 138/138 [00:01<00:00, 75.82it/s]00:49<07:05, 12.69it/s]\n",
      "Acc: 0.64: 100%|██████████| 138/138 [00:02<00:00, 68.07it/s]01:15<06:52, 12.37it/s]\n",
      "Acc: 0.64: 100%|██████████| 138/138 [00:01<00:00, 69.86it/s][01:41<06:34, 12.15it/s]\n",
      "Acc: 0.64: 100%|██████████| 138/138 [00:02<00:00, 67.25it/s] [02:06<05:49, 12.88it/s]\n",
      "Acc: 0.64: 100%|██████████| 138/138 [00:02<00:00, 68.22it/s] [02:32<05:50, 11.99it/s]\n",
      "Epoch: 138, Avg batch loss: 0.00:  30%|███       | 1800/6000 [02:34<06:01, 11.62it/s]\n",
      "Acc: 0.64: 100%|██████████| 277/277 [00:03<00:00, 80.76it/s]\n",
      "Acc: 0.64: 100%|██████████| 138/138 [00:01<00:00, 79.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>████▇▇▇▇▆▆▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>loss</td><td>0.00352</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.63937</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.63669</td></tr><tr><td>sst:val_acc</td><td>0.63669</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fearless-cherry-7</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/9ym243g5\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/9ym243g5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_205020-9ym243g5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_205308-3qltyq4b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3qltyq4b\" target=\"_blank\">summer-yogurt-8</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.65: 100%|██████████| 35/35 [00:01<00:00, 34.82it/s] [00:26<08:22, 11.35it/s]\n",
      "Acc: 0.65: 100%|██████████| 35/35 [00:00<00:00, 35.71it/s]0 [00:55<08:19, 10.82it/s]\n",
      "Acc: 0.65: 100%|██████████| 35/35 [00:01<00:00, 34.49it/s]0 [01:23<07:39, 11.09it/s]\n",
      "Epoch: 224, Avg batch loss: 0.00:  15%|█▌        | 900/6000 [01:24<08:01, 10.60it/s]\n",
      "Acc: 0.66: 100%|██████████| 70/70 [00:01<00:00, 37.50it/s]\n",
      "Acc: 0.65: 100%|██████████| 35/35 [00:00<00:00, 38.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>███▇▇▆▇▇▇▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>█▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>32</td></tr><tr><td>loss</td><td>0.0011</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.65882</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.65213</td></tr><tr><td>sst:val_acc</td><td>0.65213</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">summer-yogurt-8</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3qltyq4b\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/3qltyq4b</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_205308-3qltyq4b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_205442-smweda39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/smweda39\" target=\"_blank\">skilled-dream-9</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.57: 100%|██████████| 138/138 [00:01<00:00, 69.58it/s]00:23<07:44, 12.26it/s]\n",
      "Acc: 0.64: 100%|██████████| 138/138 [00:01<00:00, 69.04it/s]00:49<06:53, 13.06it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:01<00:00, 69.04it/s]01:15<06:41, 12.70it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 69.73it/s][01:41<06:26, 12.41it/s]\n",
      "Acc: 0.66: 100%|██████████| 138/138 [00:02<00:00, 68.12it/s] [02:07<06:04, 12.35it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 69.39it/s] [02:33<05:57, 11.74it/s]\n",
      "Epoch: 138, Avg batch loss: 0.00:  30%|███       | 1800/6000 [02:35<06:02, 11.59it/s]\n",
      "Acc: 0.67: 100%|██████████| 277/277 [00:03<00:00, 76.36it/s]\n",
      "Acc: 0.67: 100%|██████████| 138/138 [00:01<00:00, 81.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>███████▇▇▆▅▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁▆██▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>loss</td><td>0.00343</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.67149</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.66757</td></tr><tr><td>sst:val_acc</td><td>0.66757</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">skilled-dream-9</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/smweda39\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/smweda39</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_205442-smweda39/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/glai/dev/subset-active-learning/scripts/subset_selection/wandb/run-20220924_205729-2r0ypgq3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/2r0ypgq3\" target=\"_blank\">valiant-totem-10</a></strong> to <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:00<00:00, 36.28it/s] [00:26<08:35, 11.05it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:01<00:00, 34.45it/s]0 [00:55<08:37, 10.43it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:01<00:00, 33.97it/s]0 [01:23<07:49, 10.87it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:01<00:00, 34.71it/s]00 [01:51<07:33, 10.59it/s]\n",
      "Epoch: 299, Avg batch loss: 0.00:  20%|██        | 1200/6000 [01:52<07:30, 10.66it/s]\n",
      "Acc: 0.67: 100%|██████████| 70/70 [00:01<00:00, 37.29it/s]\n",
      "Acc: 0.67: 100%|██████████| 35/35 [00:00<00:00, 40.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>loss</td><td>█████▇▇▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sst2_final_test:accuracy</td><td>▁</td></tr><tr><td>sst2_final_valid:accuracy</td><td>▁</td></tr><tr><td>sst:val_acc</td><td>▁█▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>32</td></tr><tr><td>loss</td><td>0.00076</td></tr><tr><td>sst2_final_test:accuracy</td><td>0.67014</td></tr><tr><td>sst2_final_valid:accuracy</td><td>0.67484</td></tr><tr><td>sst:val_acc</td><td>0.67484</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">valiant-totem-10</strong>: <a href=\"https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/2r0ypgq3\" target=\"_blank\">https://wandb.ai/johnny-gary/subset-search-gpu-opt/runs/2r0ypgq3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220924_205729-2r0ypgq3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(43, 47):\n",
    "    train_ds = processed_ds[\"train\"].shuffle(seed=seed).select(range(100))\n",
    "    batch_size_comparison = BatchSizeComparisonRun(train_ds=train_ds, valid_ds=processed_ds[\"validation\"], test_ds=processed_ds[\"test\"])\n",
    "    batch_size_comparison.run_comparison(small_batch_config=small_batch_config, large_batch_config=large_batch_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('subset')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93a8110c327b3f25735ecf324002c5782191dd6b384b2003081090d3bf4c3d32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
