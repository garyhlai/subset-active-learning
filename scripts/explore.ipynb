{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Reusing dataset sst (/Users/garylai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 206.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'tokens', 'tree'],\n",
      "    num_rows: 8544\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sst2 = load_dataset(\"sst\")\n",
    "print(sst2[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find \"./train.ipynb\".\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjjoozzz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=subset_active_learning_corrected\n",
      "env: WANDB_NOTEBOOK_NAME=\"./train.ipynb\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload  \n",
    "%autoreload 2 \n",
    "\n",
    "from subset_active_learning.active_learner import ActiveLearner, ActiveLearnerConfig\n",
    "import wandb\n",
    "%env WANDB_PROJECT=subset_active_learning_corrected\n",
    "%env WANDB_NOTEBOOK_NAME=\"./train.ipynb\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_indices = 10\n",
    "sampling_sizes = (1, 2, 3, 4)\n",
    "\n",
    "all_samples = np.random.choice(pool_indices, replace=False, size=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 2, 4])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = all_samples[:1]; sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 2, 4])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample4 = all_samples[:4]; sample4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4, 8]), array([2, 0]), array([4, 2]), array([5, 2])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 5])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_datasets = []\n",
    "for training_size in (1000, 2000, 3000, 4000):\n",
    "    with open(f\"./debug/train_ds_random_sampling_{training_size}.pkl\", \"rb\") as f:\n",
    "        train_ds = pickle.load(f)\n",
    "        train_datasets.append(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 66])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets[0][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 66])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets[3][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.eq(train_datasets[3][\"input_ids\"][0:1000], train_datasets[3][\"input_ids\"][1000:2000]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(train_datasets[3][\"input_ids\"][2000:3000], train_datasets[3][\"input_ids\"][1000:2000]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActiveLearnerConfig(max_length=66, debug=False, model_name='google/electra-small-discriminator', strategy='random_sampling', sampling_sizes=(1000, 2000, 3000, 4000), max_steps=20000, batch_size=8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ActiveLearnerConfig(debug=False, sampling_sizes=(1000, 2000, 3000, 4000), strategy=\"random_sampling\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Reusing dataset sst (/Users/garylai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 370.41it/s]\n",
      "Loading cached processed dataset at /Users/garylai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-c6c7962a7d90e9c5.arrow\n",
      "Loading cached processed dataset at /Users/garylai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-915eb92e28e9eeaf.arrow\n",
      "Loading cached processed dataset at /Users/garylai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-90df394ccb2006db.arrow\n",
      "Loading cached processed dataset at /Users/garylai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-b196b58ce5e6a588.arrow\n",
      "Loading cached processed dataset at /Users/garylai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-4a67cf392607269a.arrow\n",
      "Loading cached processed dataset at /Users/garylai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-679dc0505b9541c9.arrow\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "active_learner = ActiveLearner(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config strategy is random_sampling\n",
      "config strategy is random_sampling\n",
      "config strategy is random_sampling\n",
      "config strategy is random_sampling\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, set_seed\n",
    "\n",
    "self = active_learner\n",
    "n_new_samples = 1000\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "all_sampled_data = []\n",
    "for i in range(4):\n",
    "        sampled_data = self.sample_data(n_new_samples)\n",
    "        all_sampled_data.append(sampled_data)\n",
    "        # concatenate the sampled data with the original data\n",
    "# self.train_data_indices.extend(sampled_data)\n",
    "# train_data = self.sst2[\"train\"].select(self.train_data_indices)\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#             output_dir=\"./dir\",\n",
    "#             max_steps=self.config.max_steps if not self.config.debug else 640,\n",
    "#             evaluation_strategy=\"steps\",\n",
    "#             report_to=\"wandb\",\n",
    "#             run_name=f\"{self.config.strategy}-size-{len(self.train_data_indices)}\",\n",
    "#             eval_steps=300,\n",
    "#             learning_rate=1e-5,\n",
    "#             adam_epsilon=1e-6,\n",
    "#             warmup_ratio=0.1,\n",
    "#             weight_decay=0.01,\n",
    "#         )\n",
    "\n",
    "# train_ds = self.preprocess(train_data)\n",
    "\n",
    "# print(f\"training_args: {training_args}\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(self.config.model_name, num_labels=2)\n",
    "# trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_ds if not self.config.debug else self.debug_ds,\n",
    "#         compute_metrics=self.compute_metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n\nMismatched elements: 1000 / 1000 (100%)\nMax absolute difference: 8410\nMax relative difference: 7715.\n x: array([4046, 1870, 2029,  453,  748, 2145, 2063, 2829, 2115, 7694, 4723,\n        360, 1046, 3715, 4035, 1199, 3404, 3096, 7435, 3534, 6567, 8131,\n       6790, 5544,  811, 6869, 1803, 5296, 1010, 5972, 7197, 2498, 2215,...\n y: array([3140, 5617, 6480, 3721, 6066, 7721, 3577, 8036, 7712, 7285, 3310,\n       6560, 6314, 2742, 4432, 2120, 5043, 8088, 3997, 1256, 2740, 6403,\n       2701, 7049, 2862, 6198, 1680, 7936, 2358, 3645, 7396, 5159,  907,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/garylai/Dev/subset-active-learning/explore.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/garylai/Dev/subset-active-learning/explore.ipynb#ch0000011?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/garylai/Dev/subset-active-learning/explore.ipynb#ch0000011?line=2'>3</a>\u001b[0m np\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_array_almost_equal(all_sampled_data[\u001b[39m0\u001b[39;49m], all_sampled_data[\u001b[39m1\u001b[39;49m])\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/numpy/testing/_private/utils.py:844\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=839'>840</a>\u001b[0m         err_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(remarks)\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=840'>841</a>\u001b[0m         msg \u001b[39m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=841'>842</a>\u001b[0m                             verbose\u001b[39m=\u001b[39mverbose, header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=842'>843</a>\u001b[0m                             names\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m), precision\u001b[39m=\u001b[39mprecision)\n\u001b[0;32m--> <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=843'>844</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=844'>845</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/numpy/testing/_private/utils.py?line=845'>846</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n\nMismatched elements: 1000 / 1000 (100%)\nMax absolute difference: 8410\nMax relative difference: 7715.\n x: array([4046, 1870, 2029,  453,  748, 2145, 2063, 2829, 2115, 7694, 4723,\n        360, 1046, 3715, 4035, 1199, 3404, 3096, 7435, 3534, 6567, 8131,\n       6790, 5544,  811, 6869, 1803, 5296, 1010, 5972, 7197, 2498, 2215,...\n y: array([3140, 5617, 6480, 3721, 6066, 7721, 3577, 8036, 7712, 7285, 3310,\n       6560, 6314, 2742, 4432, 2120, 5043, 8088, 3997, 1256, 2740, 6403,\n       2701, 7049, 2862, 6198, 1680, 7936, 2358, 3645, 7396, 5159,  907,..."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.testing.assert_array_almost_equal(all_sampled_data[0], all_sampled_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from /Users/garylai/Dev/subset-active-learning/results/checkpoints/random_sampling_4000_1500).\n",
      "You are resuming training from a checkpoint trained with 4.19.2 of Transformers but your current version is 4.19.1. This is not recommended and could yield to errors or unwanted behaviors.\n"
     ]
    }
   ],
   "source": [
    "trainer._load_from_checkpoint(\"/Users/garylai/Dev/subset-active-learning/results/checkpoints/random_sampling_4000_1500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 2000\n",
       " }),\n",
       " None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset, trainer.eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 1000 new samples\n",
      "Sampling 1000 new samples\n",
      "Sampling 1000 new samples\n",
      "Sampling 1000 new samples\n"
     ]
    }
   ],
   "source": [
    "for i, sampling_size in enumerate(self.config.sampling_sizes):\n",
    "    n_new_samples = (\n",
    "        sampling_size if i == 0 else self.config.sampling_sizes[i] - self.config.sampling_sizes[i - 1]\n",
    "    )\n",
    "    print(f\"Sampling {n_new_samples} new samples\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e5b6cb28458d38a3a51ae1fcc17b14fe2e0ac931760af92f410b7c2c57674e6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
