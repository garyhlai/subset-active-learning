{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb106d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a940ada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 15:25:51.221289: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-17 15:25:51.221346: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from typing import Any, Optional\n",
    "from pydantic import BaseModel, Extra, Field\n",
    "from transformers import TrainingArguments, AutoModel, AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "import numpy as np\n",
    "import json\n",
    "import datasets\n",
    "import wandb\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('./set_transformer/')\n",
    "from subset_active_learning.subset_selection import select, preprocess\n",
    "from subset_active_learning.active_learning.subset_classifier import get_df_from_db\n",
    "\n",
    "import torch.nn as nn\n",
    "from modules import SAB, PMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2abda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = 'sst2'\n",
    "test_dataset = None\n",
    "eval_mapping = '[]'\n",
    "num_labels = 2\n",
    "valid_split = 'validation'\n",
    "test_split = 'validation'\n",
    "\n",
    "db_path = \"./temp.db\"\n",
    "seed = 0\n",
    "pool_size = 1000\n",
    "search_size = 100\n",
    "warmup_runs = 500\n",
    "annealing_runs = 1000\n",
    "wandb_project = 'sst_search_test'\n",
    "wandb_entity = 'johntzwei'\n",
    "\n",
    "model_card = \"roberta-base\"\n",
    "pretraining = True\n",
    "max_steps = 5000\n",
    "eval_steps = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "# adam should default to correct_bias = True\n",
    "adam_epsilon = 1e-6\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "max_grad_norm = 1.0\n",
    "warmup_ratio = 0.0\n",
    "weight_decay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a0dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 2000   # dataset dependent\n",
    "embedding_dim = 768\n",
    "enc_num_layers = 3\n",
    "enc_dim = 32\n",
    "enc_num_heads = 32\n",
    "dec_dim = 32\n",
    "dec_num_heads = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ceb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '../../scripts/active_learning/sst_random_fixed_small_validation.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9eac97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = select.SubsetTrainingArguments(model_card=model_card,\n",
    "                                               num_labels=num_labels,\n",
    "                                               eval_mapping=json.loads(eval_mapping),\n",
    "                                               pretraining=pretraining,\n",
    "                                               max_steps=max_steps,\n",
    "                                               eval_steps=eval_steps,\n",
    "                                               learning_rate=learning_rate,\n",
    "                                               batch_size=batch_size,\n",
    "                                               adam_epsilon=adam_epsilon,\n",
    "                                               adam_beta1=adam_beta1,\n",
    "                                               adam_beta2=adam_beta2,\n",
    "                                               max_grad_norm=max_grad_norm,\n",
    "                                               warmup_ratio=warmup_ratio,\n",
    "                                               weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9fd6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_from_db(db_path)\n",
    "df = df.iloc[1:]\n",
    "# maybe the center is too dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be88f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtilityDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, db_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = get_df_from_db(db_path)\n",
    "        # self.df = df[(df.objective < 0.75567) | (df.objective > 0.77111) | (np.random.random(len(df)) > 0.5)]\n",
    "        \n",
    "        self.data = []\n",
    "        for i, row in self.df.iterrows():\n",
    "            if row['objective'] == 0.0:\n",
    "                continue\n",
    "            self.data.append((json.loads(row['indexes']), row['objective']))\n",
    "        print('Subset size:', len((self.data[0][0])))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "232282d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset size: 100\n"
     ]
    }
   ],
   "source": [
    "dataset = DataUtilityDataset(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1abc9868",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07afebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallSetTransformer(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, enc_num_layers, enc_dim, enc_num_heads, dec_dim, dec_num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        args = [embedding,\n",
    "            SAB(dim_in=embedding_dim, dim_out=enc_dim, num_heads=enc_num_heads),]\n",
    "        \n",
    "        for i in range(enc_num_layers - 1):\n",
    "            args.append(SAB(dim_in=enc_dim, dim_out=enc_dim, num_heads=enc_num_heads))\n",
    "        \n",
    "        self.enc = nn.Sequential(*args)\n",
    "        self.dec = nn.Sequential(\n",
    "            PMA(dim=dec_dim, num_heads=dec_num_heads, num_seeds=1),\n",
    "            nn.Linear(in_features=dec_dim, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        x = self.dec(x)\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a57cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, val_dataset, tolerance=1):\n",
    "    steps = 0\n",
    "    epochs = 0\n",
    "    best_acc = None\n",
    "    patience = 0\n",
    "    pbar = tqdm(total=params.max_steps)\n",
    "    \n",
    "    wandb_run = wandb.init(project='sst_set_transformer', entity='johntzwei', tags=[])\n",
    "    \n",
    "    def collate_fn(list_items):\n",
    "        x = []\n",
    "        y = []\n",
    "        for x_, y_ in list_items:\n",
    "            x.append(x_)\n",
    "            y.append(y_)\n",
    "        return torch.LongTensor(x), torch.Tensor(y)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=params.batch_size, pin_memory=True, collate_fn=collate_fn)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=True, batch_size=params.batch_size, pin_memory=True, collate_fn=collate_fn)\n",
    "    it = iter(train_dataloader)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=params.learning_rate, betas=(params.adam_beta1, params.adam_beta2), eps=params.adam_epsilon, weight_decay=params.weight_decay)\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    \n",
    "    \n",
    "    while steps < params.max_steps:\n",
    "        # training\n",
    "        model.train()\n",
    "        total_loss = 0.\n",
    "        try:\n",
    "            batch = next(it)\n",
    "        except:\n",
    "            epochs += 1\n",
    "            it = iter(train_dataloader)\n",
    "            batch = next(it)\n",
    "        steps += 1\n",
    "\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        loss = criterion(model(x), y)\n",
    "        wandb.log({'loss' : loss})\n",
    "        total_loss += loss.cpu()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), params.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_description('Epoch: %d, Avg batch loss: %.2f' % (epochs, total_loss / steps))\n",
    "        pbar.update(1)\n",
    "\n",
    "        if steps % params.eval_steps == 0:\n",
    "            model.eval()\n",
    "            corr, avg_loss = evaluate(model, val_dataloader, eval_mapping={})\n",
    "            wandb.log({'sst:val_spearman' : corr})\n",
    "            wandb.log({'sst:val_loss' : avg_loss})\n",
    "            # early stopping\n",
    "            if not best_acc or corr > best_acc:\n",
    "                best_acc = corr\n",
    "            else:\n",
    "                patience += 1\n",
    "            if patience >= tolerance:\n",
    "                break\n",
    "\n",
    "def evaluate(model, val_dataloader, eval_mapping: list):\n",
    "    model.eval()\n",
    "    val_pbar = tqdm(total=len(val_dataloader))\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    \n",
    "    losses = []\n",
    "    ys, ys_ = [], []\n",
    "    for batch in val_dataloader:\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_ = model(x)\n",
    "            loss = criterion(y_, y)\n",
    "        losses.append(loss.cpu())\n",
    "        \n",
    "        ys.extend(y.tolist())\n",
    "        ys_.extend(y_.tolist())\n",
    "        val_pbar.update(1)\n",
    "        \n",
    "    avg_loss = np.mean(losses)\n",
    "    ys_ = sum(ys_, [])\n",
    "    corr = np.corrcoef(np.array(ys), np.array(ys_))[0, 1]\n",
    "    val_pbar.set_description('Correlation: %.2f, Avg. loss: %.2f' % (corr, avg_loss))\n",
    "    \n",
    "        \n",
    "    plt.figure()\n",
    "    sns.scatterplot(ys, ys_)\n",
    "    \n",
    "    return corr, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0099c52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2772"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d2796dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aaf961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "else:\n",
    "    embedding = torch.load('./trained-emb_downsample.pt')\n",
    "    embedding.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d183d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b2c18105b04fe9a0608f6492999172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohntzwei\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "2022-10-17 15:26:01.441919: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-17 15:26:01.441957: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/johntzwei/sst_set_transformer/runs/2y6gqvvu\" target=\"_blank\">distinctive-snow-226</a></strong> to <a href=\"https://wandb.ai/johntzwei/sst_set_transformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnny/.conda/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/johnny/.conda/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e90a54b5494aed896b1070cbfea42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnny/.conda/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48321/3592870243.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_48321/4192494581.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, val_dataset, tolerance)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mcorr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sst:val_spearman'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sst:val_loss'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48321/4192494581.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, val_dataloader, eval_mapping)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "model = SmallSetTransformer(embedding, embedding_dim, enc_num_layers, enc_dim, enc_num_heads, dec_dim, dec_num_heads)\n",
    "model.to(device)\n",
    "\n",
    "# train_size = 1000\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset)-train_size])\n",
    "train_dataset = dataset[:1000]\n",
    "val_dataset = dataset[1000:]\n",
    "\n",
    "train(model, train_dataset, val_dataset, tolerance=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'set_transformer.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
