{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import sqlalchemy \n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from pydantic.dataclasses import dataclass\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Name : states\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"./subset_selection/sst_results.db\"\n",
    "with sqlite3.connect(DB_PATH) as conn: \n",
    "    #Now in order to read in pandas dataframe we need to know table name\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    table_name = cursor.fetchall()[0][0]\n",
    "    print(f\"Table Name : {table_name}\")\n",
    "\n",
    "    df = pd.read_sql_query(f'SELECT * FROM {table_name}', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indexes</th>\n",
       "      <th>objective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[787, 882, 40, 341, 477, 454, 227, 334, 837, 2...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[787, 882, 40, 341, 477, 454, 227, 334, 837, 2...</td>\n",
       "      <td>0.690498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[787, 882, 40, 341, 477, 454, 227, 334, 837, 2...</td>\n",
       "      <td>0.697738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[787, 882, 40, 341, 477, 454, 227, 334, 837, 2...</td>\n",
       "      <td>0.690950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[787, 882, 40, 341, 477, 454, 227, 334, 837, 2...</td>\n",
       "      <td>0.709502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8566</th>\n",
       "      <td>[517, 882, 82, 341, 477, 381, 611, 377, 837, 2...</td>\n",
       "      <td>0.747964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8567</th>\n",
       "      <td>[48, 882, 82, 341, 477, 381, 611, 377, 837, 21...</td>\n",
       "      <td>0.752941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>[517, 882, 82, 341, 477, 381, 611, 377, 837, 2...</td>\n",
       "      <td>0.742081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8569</th>\n",
       "      <td>[517, 882, 82, 341, 477, 381, 611, 377, 837, 2...</td>\n",
       "      <td>0.751131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8570</th>\n",
       "      <td>[517, 882, 82, 341, 477, 381, 611, 377, 837, 2...</td>\n",
       "      <td>0.743439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                indexes  objective\n",
       "0     [787, 882, 40, 341, 477, 454, 227, 334, 837, 2...   0.000000\n",
       "1     [787, 882, 40, 341, 477, 454, 227, 334, 837, 2...   0.690498\n",
       "2     [787, 882, 40, 341, 477, 454, 227, 334, 837, 2...   0.697738\n",
       "3     [787, 882, 40, 341, 477, 454, 227, 334, 837, 2...   0.690950\n",
       "4     [787, 882, 40, 341, 477, 454, 227, 334, 837, 2...   0.709502\n",
       "...                                                 ...        ...\n",
       "8566  [517, 882, 82, 341, 477, 381, 611, 377, 837, 2...   0.747964\n",
       "8567  [48, 882, 82, 341, 477, 381, 611, 377, 837, 21...   0.752941\n",
       "8568  [517, 882, 82, 341, 477, 381, 611, 377, 837, 2...   0.742081\n",
       "8569  [517, 882, 82, 341, 477, 381, 611, 377, 837, 2...   0.751131\n",
       "8570  [517, 882, 82, 341, 477, 381, 611, 377, 837, 2...   0.743439\n",
       "\n",
       "[8571 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_subset_idx = df['objective'].idxmax()\n",
    "\n",
    "optimal_subset_data_indices = set(json.loads(df.iloc[optimal_subset_idx].indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the unique number of data points in each subset\n",
    "indexes_list = list(map(json.loads, df[\"indexes\"].to_list()))\n",
    "indexes_list = [item for sublist in indexes_list for item in sublist]\n",
    "indexes_set = set(indexes_list)\n",
    "indexes = list(map(json.loads, df[\"indexes\"].to_list()))\n",
    "unique_index_counts = list(map(lambda x: len(set(x)), indexes))\n",
    "subset_sizes = Counter(unique_index_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: sst/default\n",
      "Reusing dataset sst (/Users/garylai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n",
      "100%|██████████| 3/3 [00:00<00:00, 309.03it/s]\n",
      "Loading cached shuffled indices for dataset at /Users/garylai/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff/cache-db8800a2b70639e2.arrow\n"
     ]
    }
   ],
   "source": [
    "sst2 = load_dataset(\"sst\")\n",
    "data_pool = sst2[\"train\"].shuffle(seed=0).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class OptimalSubsetClassifierConfig:\n",
    "    max_length: int = 66\n",
    "    debug: bool = False\n",
    "    model_name: str = \"google/electra-small-discriminator\"\n",
    "    batch_size: int = 8\n",
    "\n",
    "config = OptimalSubsetClassifierConfig()\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    def tokenize_func(examples, idx):\n",
    "            tokenized = tokenizer(\n",
    "                examples[\"sentence\"], padding=\"max_length\", max_length=config.max_length, truncation=True\n",
    "            )\n",
    "            tokenized[\"labels\"] = 1 if idx in optimal_subset_data_indices else 0\n",
    "            return tokenized\n",
    "\n",
    "\n",
    "    ds = data.map(\n",
    "        tokenize_func,\n",
    "        remove_columns=data.column_names,\n",
    "        batched=False,\n",
    "        with_indices=True\n",
    "    )\n",
    "\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2016.21ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds = preprocess(data_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert int(ds[\"labels\"][3]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the labels are correct\n",
    "for idx, label in enumerate(ds[\"labels\"]):\n",
    "    if idx in optimal_subset_data_indices:\n",
    "        assert int(label) == 1\n",
    "    else:\n",
    "        assert int(label) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2023,  2003,  1996,  2785,  1997,  3395,  3043,  2008,  2071,\n",
       "          2061,  4089,  2031,  2042, 20054,  2011,  1037,  8276, 12127,  1010,\n",
       "          2021,  1037, 16363,  2015,  3084,  1996,  2157,  9804,  2012,  2296,\n",
       "          2735,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_to_pos_ratio = (len(data_pool) - len(optimal_subset_data_indices)) // len(optimal_subset_data_indices)\n",
    "neg_to_pos_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set, datasets.arrow_dataset.Dataset)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimal_subset_data_indices), type(data_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pool_indices = set(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_optimal_subset_data_indices = data_pool_indices - optimal_subset_data_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_optimal_subset_data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split points positive: (77, 86)\n",
      "Split points negative: (723, 814)\n"
     ]
    }
   ],
   "source": [
    "optimal_subset_data_indices_ls = list(optimal_subset_data_indices)\n",
    "non_optimal_subset_data_indices_ls = list(non_optimal_subset_data_indices)\n",
    "\n",
    "\n",
    "# get the positive examples\n",
    "split_points = (round(len(optimal_subset_data_indices_ls) * 0.8), round(len(optimal_subset_data_indices_ls) * 0.9))\n",
    "print(f\"Split points positive: {split_points}\")\n",
    "\n",
    "train_pos_indices = optimal_subset_data_indices_ls[:split_points[0]]\n",
    "valid_pos_indices = optimal_subset_data_indices_ls[split_points[0]:split_points[1]]\n",
    "test_pos_indices = optimal_subset_data_indices_ls[split_points[1]:]\n",
    "\n",
    "# get the negative examples\n",
    "split_points = (round(len(non_optimal_subset_data_indices_ls) * 0.8), round(len(non_optimal_subset_data_indices_ls) * 0.9))\n",
    "print(f\"Split points negative: {split_points}\")\n",
    "\n",
    "train_neg_indices = non_optimal_subset_data_indices_ls[:split_points[0]]\n",
    "valid_neg_indices = non_optimal_subset_data_indices_ls[split_points[0]:split_points[1]]\n",
    "test_neg_indices = non_optimal_subset_data_indices_ls[split_points[1]:]\n",
    "\n",
    "# combine\n",
    "train_indices = train_pos_indices + train_neg_indices\n",
    "valid_indices = valid_pos_indices + valid_neg_indices\n",
    "test_indices = test_pos_indices + test_neg_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 100, 100)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_indices), len(valid_indices), len(test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there are no overlaps between the three datasets\n",
    "assert len(set(train_indices+valid_indices+test_indices)) == len(train_indices+valid_indices+test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds.select(train_indices)\n",
    "valid_ds = ds.select(valid_indices)\n",
    "test_ds = ds.select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 100, 100)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(valid_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datasets.arrow_dataset.Dataset,\n",
       " datasets.arrow_dataset.Dataset,\n",
       " datasets.arrow_dataset.Dataset)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds), type(valid_ds), type(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_test_split() got an unexpected keyword argument 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/garylai/Dev/subset-active-learning/train_subset_classifier.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/garylai/Dev/subset-active-learning/train_subset_classifier.ipynb#ch0000028?line=0'>1</a>\u001b[0m \u001b[39m# 90% train, 10% test + validation\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/garylai/Dev/subset-active-learning/train_subset_classifier.ipynb#ch0000028?line=1'>2</a>\u001b[0m train_testvalid \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mtrain_test_split(test\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n",
      "File \u001b[0;32m~/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:499\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=491'>492</a>\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=492'>493</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=493'>494</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=494'>495</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=495'>496</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=496'>497</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=497'>498</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=498'>499</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=499'>500</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=500'>501</a>\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/fingerprint.py?line=451'>452</a>\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/fingerprint.py?line=452'>453</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/fingerprint.py?line=453'>454</a>\u001b[0m             )\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/fingerprint.py?line=455'>456</a>\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/fingerprint.py?line=457'>458</a>\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/fingerprint.py?line=459'>460</a>\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/garylai/Dev/subset-active-learning/.venv/lib/python3.8/site-packages/datasets/fingerprint.py?line=461'>462</a>\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: train_test_split() got an unexpected keyword argument 'test'"
     ]
    }
   ],
   "source": [
    "# 90% train, 10% test + validation\n",
    "train_testvalid = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "# # Split the 10% test + valid in half test, half valid\n",
    "# test_valid = train_test_dataset['test'].train_test_split(test=0.5)\n",
    "# # gather everyone if you want to have a single DatasetDict\n",
    "# train_test_valid_dataset = DatasetDict({\n",
    "#     'train': train_testvalid['train'],\n",
    "#     'test': test_valid['test'],\n",
    "#     'valid': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train binary classifier "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e5b6cb28458d38a3a51ae1fcc17b14fe2e0ac931760af92f410b7c2c57674e6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
